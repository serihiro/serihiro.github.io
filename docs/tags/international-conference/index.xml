<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>International Conference on Kazuhiro Serizawa&#39;s profile page</title>
    <link>https://serihiro.github.io/tags/international-conference/</link>
    <description>Recent content in International Conference on Kazuhiro Serizawa&#39;s profile page</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://serihiro.github.io/tags/international-conference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Accelerating Machine Learning I/O by Overlapping Data Staging and Mini-batch Generations</title>
      <link>https://serihiro.github.io/publications/bdcat/</link>
      <pubDate>Wed, 04 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://serihiro.github.io/publications/bdcat/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://speakerdeck.com/serihiro/o-by-overlapping-data-staging-and-mini-batch-generations&#34;&gt;The slide of this talk&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The training dataset used in deep neural networks (DNNs) keeps on increasing. When a training dataset grows larger, the reading performance of such a large training dataset becomes a problem. A high-performance computing (HPC) cluster has high performance I/O storage devices, for example, NVMe SSD, as local storage on each compute node. This high-performance I/O storage can mitigate the I/O bottleneck. However, such storage devices provide only temporary storage, therefore the users have to copy the training dataset from shared storage (such as Lustre) into local storage. Large datasets (over a few hundred GiB) takes a long time to copy the datasets between local storage and shared storage. To solve this problem, we propose a method to conceal the time spent on copying dataset to local storage by overlapping the copying and reading of training data. We implemented the proposed method at the machine learning framework Chainer. The results of our experiments showed that the read I/O bandwidth of our method improved from 1.38 times to 6.19 times compared with reading the dataset from Lustre directly using Chainer standard class. Moreover, evaluation of data parallel training showed that our method improved the performance from 1.26 times to 1.74 times for the same comparison.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
